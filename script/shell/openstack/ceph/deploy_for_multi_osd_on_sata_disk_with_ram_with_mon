#!/bin/bash

set -e
set -u

if [[ "" == "$1" ]];then
    echo "args error..."
    exit 1
fi

cluster='ceph'
ceph_root_dir='/var/lib/ceph'
ceph_conf='/etc/ceph/ceph.conf'
data_disks=$1
data_split_num=3
mon_addr=192.168.66.117
public_network=192.168.66.0/24

pkill ceph-mon || true
# cluster wide parameters
mkdir -p ${ceph_root_dir}/log
cat > $ceph_conf <<EOF
[global]
fsid = $(uuidgen)
# osd_crush_chooseleaf_type = 1
auth_cluster_required = none
auth_service_required = none
auth_client_required = none
osd_pool_default_size = 2
public_network = ${public_network}
EOF

# single monitor
mon_data="${ceph_root_dir}/mon/${cluster}-0"
mkdir -p $mon_data

cat >> $ceph_conf <<EOF
[mon.0]
mon_data = ${mon_data}
mon_addr = ${mon_addr}
EOF

ceph-mon --id 0 --mkfs --keyring /dev/null
touch ${mon_data}/keyring
touch $mon_data/sysvinit
service ceph start mon
# ceph-mon --id 0 

### 分区
# 对各个磁盘分区
for disk in $data_disks; do
    disk=/dev/${disk}
    sgdisk -o ${disk}
    data_disk_size=`fdisk -l $disk  | grep "Disk ${disk}:" | awk '{print $5}'`
    # 转换成KB
    data_disk_size=$(( $data_disk_size/1024 ))
    split_disk_size=$(( $data_disk_size/$data_split_num ))
    ptype_tobe="89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be"
    for ((partnum=1;partnum<=$data_split_num;partnum++));do
        data_uuid=`uuidgen`
        start=$(( ($partnum-1)*$split_disk_size ))
        end=$(( $start+$split_disk_size-1 ))
        if [[ "$partnum" == "$data_split_num" ]]; then
            sgdisk --largest-new=$partnum \
                --change-name="$partnum:ceph data $partnum" \
                --partition-guid="$partnum:${data_uuid}" \
                --typecode="$partnum:${ptype_tobe}" \
                --mbrtogpt -- ${disk}
            break
        fi
        sgdisk --new="$partnum:${start}K:${end}K" \
            --change-name="$partnum:ceph data $partnum" \
            --partition-guid="$partnum:${data_uuid}" \
            --typecode="$partnum:${ptype_tobe}" \
            --mbrtogpt -- ${disk}
    done
done

# 创建journal ramdisk分区
osd_journal_size=5000
journal_dir=${ceph_root_dir}/osd/ramdisk
mkdir -p $journal_dir
mount tmpfs $journal_dir -t tmpfs 

### 创建各个osd
# pkill ceph-osd || true
host=`hostname`
data_disk_num=0
for disk in $data_disks; do
    for ((partnum=1;partnum<=$data_split_num;partnum++));do
        osd_id=$(ceph osd create)

        osd_data="${ceph_root_dir}/osd/${cluster}-${osd_id}"
        disk_part=${disk}${partnum}
        mkdir -p $osd_data
        mkfs -t xfs -f -i size=2048 /dev/$disk_part
        echo "osd${osd_id} use /dev/$disk_part, mounted on $osd_data"
        mount -t xfs -o rw,noatime,inode64,logbsize=256k,delaylog /dev/$disk_part $osd_data

        # prepare for journal
        journal_real_path=${journal_dir}/osd.${osd_id}
        touch $journal_real_path
        journal_path=${osd_data}/journal
        ln -s $journal_real_path $journal_path

        cat >> $ceph_conf <<EOF
[osd.${osd_id}]
osd_data = ${osd_data}
osd_journal = ${journal_path}
osd_journal_size = ${osd_journal_size}
journal_dio = false
EOF
        ceph osd crush add osd.${osd_id} 1 root=default host=${host}
        ceph-osd --id ${osd_id} --mkjournal --mkfs
        touch $osd_data/sysvinit
        service ceph start osd.${osd_id}
        # ceph-osd --id ${osd_id}
    done
    data_disk_num=$(( $data_disk_num+1 ))
done


# check that it works
# rados --pool data put group /etc/group
# rados --pool data get group ${ceph_root_dir}/group
# diff /etc/group ${ceph_root_dir}/group
# ceph osd tree

